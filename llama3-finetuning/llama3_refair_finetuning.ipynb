{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if you are using a Kaggle notebook (pay attention to the version of torch!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip\n",
    "%pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install transformers==4.41.2\n",
    "%pip install datasets==2.19.2\n",
    "%pip install accelerate==0.30.1\n",
    "%pip install openpyxl==3.1.3\n",
    "%pip install matplotlib==3.7.5\n",
    "%pip install scikit-learn==1.2.2\n",
    "%pip install tensorboard==2.15.1\n",
    "%pip install bitsandbytes==0.43.1\n",
    "%pip install peft==0.11.1\n",
    "%pip install trl==0.9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running everything locally, run this. Remember to create a venv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip\n",
    "%pip install torch==2.3.1\n",
    "%pip install transformers==4.42.3\n",
    "%pip install datasets==2.20.0\n",
    "%pip install accelerate==0.31.0\n",
    "%pip install colored==2.2.4\n",
    "%pip install openpyxl==3.1.5\n",
    "%pip install matplotlib==3.9.1\n",
    "%pip install scikit-learn==1.5.1\n",
    "%pip install seaborn==0.13.2\n",
    "%pip install tensorboard==2.17.0\n",
    "%pip install bitsandbytes==0.43.1\n",
    "%pip install peft==0.11.1\n",
    "%pip install trl==0.9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to Hugging Face (this is required to download the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same seed everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "MY_SEED = 1337\n",
    "\n",
    "random.seed(MY_SEED)\n",
    "np.random.seed(MY_SEED)\n",
    "torch.manual_seed(MY_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and the tokenizer, using an appropriate BitsAndBytes configuration for quantization (4bit). \n",
    "\n",
    "I still have to decide whether to use flash attention or not... Let's keep it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ") \n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast = True)\n",
    "tokenizer.add_special_tokens({\"pad_token\" : \"<|pad|>\"})\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    #attn_implementation = \"flash_attention_2\",\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset download & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [ReFair US dataset](https://anonymous.4open.science/r/ReFAIR-Toward-a-Context-Aware-Fairness-Recommender-in-Requirement-Engineering-18C7/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O synthetic_user_stories.xlsx https://anonymous.4open.science/r/ReFAIR-Toward-a-Context-Aware-Fairness-Recommender-in-Requirement-Engineering-18C7/3.%20Source%20Code/ReFair/datasets/Synthetic%20User%20Stories.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the .xlsx file to a .csv one, then load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = pd.read_excel(\"synthetic_user_stories.xlsx\", sheet_name = \"Dataset\")\n",
    "df.to_csv(\"synthetic_user_stories.csv\", index = False)\n",
    "dataset = load_dataset(\"csv\", data_files = \"synthetic_user_stories.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][ : 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe using the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for item in dataset[\"train\"]:\n",
    "    rows.append(\n",
    "        {\n",
    "            \"domain_cluster\": item[\"Domain Cluster\"],\n",
    "            \"topic\": item[\"Topic\"],\n",
    "            \"domain\": item[\"Domain\"],\n",
    "            \"ml_task\": item[\"Machine Learning Task\"],\n",
    "            \"user_story\": item[\"User Story\"],\n",
    "        }\n",
    "    )\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check null values (just to be safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a \"text\" column containing the entire prompt for each element (see example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(row: dict):\n",
    "    system_message = \"You are a helpful AI assistant\"\n",
    "    user_message = f\"Considering the following machine learning technique: {row['ml_task']} in the field of machine learning. \"\\\n",
    "                   f\"Can you provide me with a specific user story for the following application domain? {row['domain']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": row['user_story']}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False)\n",
    "\n",
    "df[\"text\"] = df.apply(format_example, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a \"token_count\" column that keeps track of the number of tokens for each element (see example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(row: dict) -> int:\n",
    "    return tokenizer(row[\"text\"], add_special_tokens = True, return_length = True)[\"length\"][0]\n",
    "\n",
    "df[\"token_count\"] = df.apply(count_tokens, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"token_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of the \"token_count\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "plt.hist(df.token_count, weights = np.ones(len(df.token_count)) / len(df.token_count))\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 128\n",
    "\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(f\"Number of rows (token_count < {LIMIT}): {len(df[df.token_count < 128])}\")\n",
    "print(f\"% of rows (token_count < {LIMIT}): {(len(df[df.token_count < 128]) / len(df)) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discards prompts with a number of tokens greater than 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.token_count < 128]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.domain_cluster, bins = (np.arange(10) - 0.5), rwidth = 0.8)\n",
    "plt.xticks(rotation = 45, ha = \"right\", fontsize = 10)\n",
    "plt.xlabel('Domain cluster')\n",
    "plt.ylabel('US Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train set, validation set, test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, validation_and_test = train_test_split(df, test_size = 0.2, stratify = df[\"domain_cluster\"], random_state = MY_SEED)\n",
    "validation, test = train_test_split(validation_and_test, test_size = 0.2, stratify = validation_and_test[\"domain_cluster\"], random_state = MY_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set elements: {len(train)}, {(len(train) / len(df)) * 100}%\")\n",
    "print(f\"Validation set elements: {len(validation)}, {(len(validation) / len(df)) * 100}%\")\n",
    "print(f\"Test set elements: {len(test)}, {(len(test) / len(df)) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by = [\"domain_cluster\"])\n",
    "validation = validation.sort_values(by = [\"domain_cluster\"])\n",
    "test = test.sort_values(by = [\"domain_cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(np.concatenate([test['domain_cluster'], train['domain_cluster'], validation['domain_cluster']]))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (12, 8))\n",
    "\n",
    "axs[0].hist(train[\"domain_cluster\"], bins = (np.arange(10) - 0.5), rwidth = 0.8)\n",
    "axs[0].set_xticks(np.arange(len(unique_labels)))\n",
    "axs[0].set_xticklabels(unique_labels, rotation = 45, ha = \"right\", fontsize = 10)\n",
    "axs[0].set_xlabel(\"Domain cluster\")\n",
    "axs[0].set_ylabel(\"US Number\")\n",
    "axs[0].set_title(\"Train Dataset\")\n",
    "\n",
    "axs[1].hist(validation[\"domain_cluster\"], bins = (np.arange(10) - 0.5), rwidth=  0.8)\n",
    "axs[1].set_xticks(np.arange(len(unique_labels)))\n",
    "axs[1].set_xticklabels(unique_labels, rotation = 45, ha = \"right\", fontsize = 10)\n",
    "axs[1].set_xlabel(\"Domain cluster\")\n",
    "axs[1].set_ylabel(\"US Number\")\n",
    "axs[1].set_title(\"Validation Dataset\")\n",
    "\n",
    "axs[2].hist(test[\"domain_cluster\"], bins=(np.arange(10) - 0.5), rwidth = 0.8)\n",
    "axs[2].set_xticks(np.arange(len(unique_labels)))\n",
    "axs[2].set_xticklabels(unique_labels, rotation = 45, ha = \"right\", fontsize = 10)\n",
    "axs[2].set_xlabel(\"Domain cluster\")\n",
    "axs[2].set_ylabel(\"US Number\")\n",
    "axs[2].set_title(\"Test Dataset\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json(\"train.json\", orient = \"records\", lines = True)\n",
    "validation.to_json(\"val.json\", orient = \"records\", lines = True)\n",
    "test.to_json(\"test.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"train\": \"train.json\", \"validation\": \"val.json\", \"test\": \"test.json\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens = 128,\n",
    "    return_full_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompt(row):\n",
    "    system_message = \"You are a helpful AI assistant\"\n",
    "    user_message = f\"Considering the following machine learning technique: {row['ml_task']} in the field of machine learning. \"\\\n",
    "                   f\"Can you provide me with a specific user story for the following application domain? {row['domain']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "rows = []\n",
    "for row in tqdm(dataset[\"test\"]):\n",
    "    prompt = create_test_prompt(row)\n",
    "    outputs = pipe(prompt)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"domain_cluster\": row[\"domain_cluster\"],\n",
    "            \"topic\": row[\"topic\"],\n",
    "            \"domain\": row[\"domain\"],\n",
    "            \"ml_task\": row[\"ml_task\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"original_user_story\": row[\"user_story\"],\n",
    "            \"untrained_model_user_story\": outputs[0][\"generated_text\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "report_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_csv(\"report_temp.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = \"all-linear\", # [\"self_attn.q_proj\",\"self_attn.k_proj\",\"self_attn.v_proj\",\"self_attn.o_proj\",\"mlp.gate_proj\",\"mlp.up_proj\",\"mlp.down_proj\"]\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"./output/runs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Train the model on the generated prompts only](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"<|end_header_id|>\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a Kaggle notebook (GPU T4 x 2), you can use a higher batch size. This configuration works locally with an RTX 3060 (12 GB of VRAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "bf16 = torch.cuda.is_bf16_supported()\n",
    "fp16 = not bf16\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 128,\n",
    "    bf16 = bf16,\n",
    "    fp16 = fp16,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 0.2,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 0.2,\n",
    "    logging_steps = 10,\n",
    "    learning_rate = 1e-4,\n",
    "    warmup_ratio = 0.1,\n",
    "    save_total_limit = 2,\n",
    "    lr_scheduler_type = \"constant\",\n",
    "    report_to = \"tensorboard\",\n",
    "    save_safetensors = True,\n",
    "    seed = MY_SEED,\n",
    "    output_dir = \"./output/\",\n",
    "    dataset_kwargs = {\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    args = sft_config,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Llama-3-8B-Instruct-Refair-FAIRWAY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge LoRA with the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this step requires a large amount of VRAM. I was only able to run it on a Kaggle notebook. \n",
    "\n",
    "Otherwise you could load the quantized model and merge it with the LoRA, but I think it is very likely that the results will have a lower quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to free up some VRAM. In the worst case, manually reboot the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some VRAM\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add these imports in case something crashes and you just need to merge the model with the LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama-3-8B-Instruct-Refair-FAIRWAY\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of = 8)\n",
    "model = PeftModel.from_pretrained(model, \"Llama-3-8B-Instruct-Refair-FAIRWAY\")\n",
    "model = model.merge_and_unload(progressbar = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model and tokenizer to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Llama-3-8B-Instruct-Refair-FAIRWAY\", tokenizer = tokenizer, max_shard_size = \"5GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"Llama-3-8B-Instruct-Refair-FAIRWAY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"train\": \"train.json\", \"validation\": \"val.json\", \"test\": \"test.json\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ") \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DG266/Llama-3-8B-Instruct-Refair-FAIRWAY\", use_fast = True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"DG266/Llama-3-8B-Instruct-Refair-FAIRWAY\",\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of the fine-tuned model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens = 128,\n",
    "    return_full_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompt(row):\n",
    "    system_message = \"You are a helpful AI assistant\"\n",
    "    user_message = f\"Considering the following machine learning technique: {row['ml_task']} in the field of machine learning. \"\\\n",
    "                   f\"Can you provide me with a specific user story for the following application domain? {row['domain']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "generated_user_stories = []\n",
    "for row in tqdm(dataset[\"test\"]):\n",
    "    outputs = pipe(create_test_prompt(row))\n",
    "    generated_user_stories.append(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df[\"trained_model_user_story\"] = generated_user_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_csv(\"report.csv\", index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
