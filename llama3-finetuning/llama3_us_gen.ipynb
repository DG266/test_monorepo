{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if you are using a Kaggle notebook (pay attention to the version of torch!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U pip\n",
    "%pip install torch==2.3.1\n",
    "%pip install transformers==4.42.3\n",
    "%pip install datasets==2.20.0\n",
    "%pip install accelerate==0.31.0\n",
    "%pip install colored==2.2.4\n",
    "%pip install openpyxl==3.1.5\n",
    "%pip install matplotlib==3.9.1\n",
    "%pip install scikit-learn==1.5.1\n",
    "%pip install seaborn==0.13.2\n",
    "%pip install tensorboard==2.17.0\n",
    "%pip install bitsandbytes==0.43.1\n",
    "%pip install peft==0.11.1\n",
    "%pip install trl==0.9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to Hugging Face (this is required to download the fine-tuned model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ") \n",
    "\n",
    "model_path = \"DG266/Llama-3-8B-Instruct-Refair-FAIRWAY\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast = True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    #attn_implementation = \"flash_attention_2\",\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate User Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider all 34 domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"Biology\", \"Cardiology\", \"Computer Networks\", \"Computer Vision\", \"Demography\", \"Dermatology\", \"Economics\", \"Education\", \"Endocrinology\", \"Finance & Marketing\", \"Health\", \"Information Systems\", \"Law\", \"Library\", \"Linguistics\", \"Literature\", \"Medicine\", \"Movies\", \"Music\", \"Nephrology\", \"News\", \"Pediatrics\", \"Pharmacology\", \"Plant Science\", \"Political Science\", \"Psychology\", \"Radiology\", \"Social Media\", \"Social Networks\", \"Social Work\", \"Sociology\", \"Sport\", \"Transportation\", \"Urban Studies\"]\n",
    "len(domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and these 20 machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"adversarial learning\", \"cnn\", \"conversational agent\", \"decision tree\", \"document classification\", \"entity extraction\", \"feature selection\", \"imbalanced dataset\", \"keyword extraction\", \"k-nearest neighbor\", \"multi-label classification\", \"neural network\", \"random forest\", \"semantic similarity\", \"sentiment analysis\", \"speech to text\", \"text categorization\", \"unsupervised clustering\", \"voice recognition\", \"word embedding\"]\n",
    "len(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens = 128,\n",
    "    return_full_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(task, domain):\n",
    "    system_message = \"You are a helpful AI assistant\"\n",
    "    user_message = f\"Considering the following machine learning technique: {task} in the field of machine learning. \"\\\n",
    "                   f\"Can you provide me with a specific user story for the following application domain? {domain}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate user stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for domain in domains:\n",
    "    if domain in [\"Biology\", \"Plant Science\"]:\n",
    "        cluster = \"Biology & Botanic\"\n",
    "        topic = 1\n",
    "    elif domain in [\"Economics\", \"Finance & Marketing\"]:\n",
    "        cluster = \"Economy & Marketing\"\n",
    "        topic = 2\n",
    "    elif domain in [\"Information Systems\", \"News\"]:\n",
    "        cluster = \"Information Systems & News\"\n",
    "        topic = 3\n",
    "    elif domain in [\"Law\", \"Political Science\"]:\n",
    "        cluster = \"Law & Politics\"\n",
    "        topic = 4\n",
    "    elif domain in [\"Library\", \"Linguistics\", \"Literature\"]:\n",
    "        cluster = \"Literature & Linguistics\"\n",
    "        topic = 5\n",
    "    elif domain in [\"Cardiology\", \"Dermatology\", \"Endocrinology\", \"Health\", \"Medicine\", \"Nephrology\", \"Pediatrics\", \"Pharmacology\", \"Psychology\", \"Radiology\"]:\n",
    "        cluster = \"Medicine & Health\"\n",
    "        topic = 6\n",
    "    elif domain in [\"Demography\", \"Education\", \"Social Media\", \"Social Networks\", \"Social Work\", \"Sociology\", \"Transportation\", \"Urban Studies\"]:\n",
    "        cluster = \"Social and Urban Studies\"\n",
    "        topic = 7\n",
    "    elif domain in [\"Movies\", \"Music\", \"Sport\"]:\n",
    "        cluster = \"Sport & Entertainment\"\n",
    "        topic = 8\n",
    "    elif domain in [\"Computer Networks\", \"Computer Vision\"]:\n",
    "        cluster = \"Technical Domains\"\n",
    "        topic = 9\n",
    "\n",
    "    for task in tasks:\n",
    "        prompt = create_prompt(task, domain)\n",
    "        outputs = pipe(prompt)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"Domain Cluster\": cluster,\n",
    "                \"Topic\": topic,\n",
    "                \"Domain\": domain,\n",
    "                \"Machine Learning Task\": task,\n",
    "                \"User Story\": outputs[0][\"generated_text\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_user_stories_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_user_stories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save everything in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_user_stories_df.to_csv(\"llama3_finetuned_user_stories.csv\", index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
