{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U pip\n",
    "%pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install transformers==4.41.2\n",
    "%pip install bitsandbytes==0.43.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Llama 3 8B Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to Hugging Face (this is required to download the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and the tokenizer, using an appropriate BitsAndBytes configuration for quantization (4bit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map = \"auto\", quantization_config = bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate User Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the prompt. You have to use the [Meta Llama 3 Instruct prompt format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3).\n",
    "\n",
    "The tensor should contain these special tokens: \n",
    "- <|begin_of_text|> = 128000\n",
    "- <|start_header_id|> = 128006\n",
    "- <|end_header_id|> = 128007\n",
    "- <|eot_id|> = 128009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "user_prompt = (\n",
    "    \"Considering the following machine learning technique: \"\n",
    "    \"nearest neighbour search in the field of machine learning. \"\n",
    "    \"Can you provide me with specific user stories for the following application domains? \"\n",
    "    \"Finance and Marketing\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI assistant\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    },\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt = True, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    input_ids = model_inputs,\n",
    "    min_new_tokens = 0,\n",
    "    max_new_tokens = 1024,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_k = 50,\n",
    "    top_p = 0.9\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length : ], skip_special_tokens = False)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [TextStreamer](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextStreamer) to print the token(s) to stdout as soon as entire words are formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "_ = model.generate(\n",
    "    input_ids = model_inputs,\n",
    "    min_new_tokens = 0,\n",
    "    max_new_tokens = 1024,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_k = 50,\n",
    "    top_p = 0.9,\n",
    "    streamer = streamer\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
