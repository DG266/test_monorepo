{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, hamming_loss, f1_score, multilabel_confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "import fasttext\n",
    "from transformers import BertTokenizer\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "dataset = pd.read_excel(\"Synthetic User Stories.xlsx\") # Change dataset here \n",
    "\n",
    "labels = pd.read_excel(\"Keyword labelled.xlsx\", header=None)\n",
    "labels[2] = labels[2].apply(lambda x: x.lower())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_column = []\n",
    "for row in labels.iterrows():\n",
    "    current_labels = []\n",
    "    for label in row[1][3:]:\n",
    "        if isinstance(label, str):\n",
    "            current_labels.append(label.lower())\n",
    "    categories_column.append(current_labels)\n",
    "labels[\"Categories array\"] = categories_column\n",
    "labels[[2, \"Categories array\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "counter = 0\n",
    "for row in dataset.iterrows():\n",
    "    target.append(labels[labels[2]==row[1][\"Machine Learning Task\"].lower()][\"Categories array\"].values[0])\n",
    "    counter += 1\n",
    "dataset[\"Target\"] = target\n",
    "dataset[[\"User Story\",\"Target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast categories into lists\n",
    "dataset['Target'] = dataset['Target'].apply(lambda x: ast.literal_eval(str(x)))\n",
    "dataset['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel = MultiLabelBinarizer()\n",
    "y = multilabel.fit_transform(dataset['Target'])\n",
    "pd.DataFrame(y, columns=multilabel.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(dataset.Target.apply(tuple).value_counts().to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = ['F1-mean', 'Precision-mean', 'Recall-mean', 'Hammer-Loss-mean',\n",
    "           'F1-fold1','Precision-fold1','Recall-fold1','Hammer-Loss-fold1',\n",
    "           'F1-fold2','Precision-fold2','Recall-fold2','Hammer-Loss-fold2',\n",
    "           'F1-fold3','Precision-fold3','Recall-fold3','Hammer-Loss-fold3',\n",
    "           'F1-fold4','Precision-fold4','Recall-fold4','Hammer-Loss-fold4',\n",
    "           'F1-fold5','Precision-fold5','Recall-fold5','Hammer-Loss-fold5',\n",
    "           'F1-fold6','Precision-fold6','Recall-fold6','Hammer-Loss-fold6',\n",
    "           'F1-fold7','Precision-fold7','Recall-fold7','Hammer-Loss-fold7',\n",
    "           'F1-fold8','Precision-fold8','Recall-fold8','Hammer-Loss-fold8',\n",
    "           'F1-fold9','Precision-fold9','Recall-fold9','Hammer-Loss-fold9',\n",
    "           'F1-fold10','Precision-fold10','Recall-fold10','Hammer-Loss-fold10',\n",
    "]\n",
    "\n",
    "column = ['BinaryRelevance LogisticRegression','BinaryRelevance RandomForestClassifier','BinaryRelevance GaussianNB','BinaryRelevance LinearSVC','BinaryRelevance KNeighborsClassifier','BinaryRelevance DecisionTreeClassifier',\n",
    "          'ClassifierChain LogisticRegression','ClassifierChain RandomForestClassifier','ClassifierChain GaussianNB','ClassifierChain LinearSVC','ClassifierChain KNeighborsClassifier','ClassifierChain DecisionTreeClassifier', \n",
    "          'LabelPowerset LogisticRegression','LabelPowerset RandomForestClassifier','LabelPowerset GaussianNB','LabelPowerset LinearSVC','LabelPowerset KNeighborsClassifier','LabelPowerset DecisionTreeClassifier'\n",
    "        ]\n",
    "results = pd.DataFrame(index=indexes, columns=column)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred1 = pd.DataFrame(columns=['Real Label',\n",
    "                                  'BinaryRelevance LogisticRegression','BinaryRelevance RandomForestClassifier','BinaryRelevance GaussianNB','BinaryRelevance LinearSVC','BinaryRelevance KNeighborsClassifier','BinaryRelevance DecisionTreeClassifier',\n",
    "                                  'ClassifierChain LogisticRegression','ClassifierChain RandomForestClassifier','ClassifierChain GaussianNB','ClassifierChain LinearSVC','ClassifierChain KNeighborsClassifier','ClassifierChain DecisionTreeClassifier', \n",
    "                                  'LabelPowerset LogisticRegression','LabelPowerset RandomForestClassifier','LabelPowerset GaussianNB','LabelPowerset LinearSVC','LabelPowerset KNeighborsClassifier','LabelPowerset DecisionTreeClassifier'])\n",
    "\n",
    "for i in range(0, len(y)):\n",
    "  df_pred1.loc[i, 'Real Label'] = list(y[i])\n",
    "\n",
    "\n",
    "df_pred2 = df_pred1.copy(deep=True)\n",
    "df_pred3 = df_pred1.copy(deep=True)\n",
    "df_pred4 = df_pred1.copy(deep=True)\n",
    "df_pred5 = df_pred1.copy(deep=True)\n",
    "df_pred6 = df_pred1.copy(deep=True)\n",
    "df_pred7 = df_pred1.copy(deep=True)\n",
    "df_pred8 = df_pred1.copy(deep=True)\n",
    "df_pred9 = df_pred1.copy(deep=True)\n",
    "df_pred10 = df_pred1.copy(deep=True)\n",
    "df_pred1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "  f, axes = plt.subplots(2, 4, figsize=(25, 15))\n",
    "  axes = axes.ravel()\n",
    "  for i in range(7):\n",
    "    disp = ConfusionMatrixDisplay(cm[i])\n",
    "    disp.plot(ax=axes[i], values_format='.4g',cmap='Blues')\n",
    "    disp.ax_.set_title(list(multilabel.classes_)[i])\n",
    "    disp.im_.colorbar.remove()\n",
    "\n",
    "  f.delaxes(axes[7])\n",
    "  plt.subplots_adjust(wspace=0.25, hspace=0.10)\n",
    "  f.colorbar(disp.im_, ax=axes)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, mlb_estimator, X, y):\n",
    "\n",
    "  clf=mlb_estimator(model)\n",
    "  kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "  prec_scores = np.zeros(10)\n",
    "  rec_scores = np.zeros(10)  \n",
    "  f_scores = np.zeros(10)\n",
    "  ham_scores = np.zeros(10)\n",
    "  conf_matrix = []\n",
    "  pred = pd.DataFrame(columns=['0','1','2','3','4','5','6','7','8','9'])\n",
    "  idx = 0\n",
    "  name = str(type(clf).__name__) + \" \" + str(type(model).__name__)\n",
    "  for index, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "      print(\"Training on fold \" + str(index+1) + \"/10...\")\n",
    "      # Generate batches from indices\n",
    "      X_train, X_test, y_train, y_test = \\\n",
    "            X[ X.index.isin(train_index)], X[ X.index.isin(test_index)], y[train_index], y[test_index]\n",
    "\n",
    "      clf.fit(X_train.values, y_train)\n",
    "      clf_pred = clf.predict(X_test.values)\n",
    "      \n",
    "      for j in range (0, len(clf_pred.toarray())):\n",
    "        pred.loc[j, str(idx)] = list(clf_pred.toarray()[j])\n",
    "\n",
    "      conf_matrix.append(multilabel_confusion_matrix(y_test, clf_pred))\n",
    "      prec_scores[idx] = precision_score(y_test, clf_pred, average='micro')\n",
    "      rec_scores[idx] = recall_score(y_test, clf_pred, average='micro')\n",
    "      f_scores[idx] = f1_score(y_test, clf_pred, average='micro')\n",
    "      ham_scores[idx] = hamming_loss(y_test, clf_pred)\n",
    "      idx+=1\n",
    "\n",
    "  results.loc['F1-mean'][name] = np.mean(f_scores)\n",
    "  results.loc['Precision-mean'][name] = np.mean(prec_scores)\n",
    "  results.loc['Recall-mean'][name] = np.mean(rec_scores)\n",
    "  results.loc['Hammer-Loss-mean'][name] = np.mean(ham_scores)\n",
    "\n",
    "  for i in range (0,10):\n",
    "      f1 = \"F1-fold\"\n",
    "      prec = \"Precision-fold\"\n",
    "      rec = \"Recall-fold\"\n",
    "      ham = \"Hammer-Loss-fold\"\n",
    "      results.loc[f1+str(i+1)][name] = f_scores[i]\n",
    "      results.loc[prec+str(i+1)][name] = prec_scores[i]\n",
    "      results.loc[rec+str(i+1)][name] = rec_scores[i]\n",
    "      results.loc[ham+str(i+1)][name] = ham_scores[i]\n",
    "\n",
    "\n",
    "  for i in range(0, len(pred)):\n",
    "    df_pred1.loc[i, name] = pred.iloc[i]['0']\n",
    "    df_pred2.loc[i, name] = pred.iloc[i]['1']\n",
    "    df_pred3.loc[i, name] = pred.iloc[i]['2']\n",
    "    df_pred4.loc[i, name] = pred.iloc[i]['3']\n",
    "    df_pred5.loc[i, name] = pred.iloc[i]['4']\n",
    "    df_pred6.loc[i, name] = pred.iloc[i]['5']\n",
    "    df_pred7.loc[i, name] = pred.iloc[i]['6']\n",
    "    df_pred8.loc[i, name] = pred.iloc[i]['7']\n",
    "    df_pred9.loc[i, name] = pred.iloc[i]['8']\n",
    "    df_pred10.loc[i, name] = pred.iloc[i]['9']\n",
    "\n",
    "  return prec_scores, rec_scores, f_scores, ham_scores, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainSetFastText():\n",
    "    ft_model = fasttext.load_model(\"fasttext_model.bin\")\n",
    "    traindata = []\n",
    "    for msg in dataset['User Story']:\n",
    "        traindata.append(ft_model.get_sentence_vector(msg))\n",
    "    traindata = pd.DataFrame(traindata)\n",
    "    traindata.columns = traindata.columns.astype(str)\n",
    "    return traindata\n",
    "\n",
    "def getTrainSetTFIDF():\n",
    "    countvec = CountVectorizer(max_features=100)\n",
    "    bow = countvec.fit_transform(dataset['User Story']).toarray()\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(bow).toarray()\n",
    "    training_data = pd.DataFrame(X)\n",
    "    training_data.columns = training_data.columns.astype(str)\n",
    "    return training_data\n",
    "\n",
    "def getTrainSetBERT():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenized_data = tokenizer(dataset['User Story'].tolist(), padding=True, truncation=True, max_length=100)\n",
    "    traindata = []\n",
    "    for msg in tokenized_data['input_ids']:\n",
    "        traindata.append(msg)\n",
    "    traindata = pd.DataFrame(traindata)\n",
    "    traindata.columns = traindata.columns.astype(str)\n",
    "    return traindata\n",
    "\n",
    "def getTrainSetWord2Vec():\n",
    "    # word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "    # word2vec_vectors.save('word2vec_model.bin')\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('word2vec-google-news-300.bin', binary=True)\n",
    "    traindata = []\n",
    "    for msg in dataset['User Story']:\n",
    "        words = msg.split()\n",
    "        vecs = []\n",
    "        for word in words:\n",
    "            if word in w2v_model:\n",
    "                vecs.append(w2v_model[word][:100])\n",
    "        if vecs:\n",
    "            vec_avg = sum(vecs) / len(vecs)\n",
    "        else:\n",
    "            vec_avg = [0] * 100\n",
    "        traindata.append(vec_avg)\n",
    "\n",
    "    traindata = pd.DataFrame(traindata)\n",
    "    traindata.columns = traindata.columns.astype(str)\n",
    "    return traindata\n",
    "\n",
    "def getTrainSetGlove():\n",
    "    glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "    traindata = []\n",
    "    for msg in dataset['User Story']:\n",
    "        words = msg.split()\n",
    "        vecs = []\n",
    "        for word in words:\n",
    "            if word in glove_vectors:\n",
    "                vecs.append(glove_vectors[word])\n",
    "        if vecs:\n",
    "            vec_avg = sum(vecs) / len(vecs)\n",
    "        else:\n",
    "            vec_avg = [0] * 100\n",
    "        traindata.append(vec_avg)\n",
    "\n",
    "    traindata = pd.DataFrame(traindata)\n",
    "    traindata.columns = traindata.columns.astype(str)\n",
    "    return traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getTrainSetBERT()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BinaryRelevance**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, BinaryRelevance, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, BinaryRelevance, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, BinaryRelevance, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, BinaryRelevance, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, BinaryRelevance, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, BinaryRelevance, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ClassifierChain**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, ClassifierChain, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, ClassifierChain, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####2.4.2.3) Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, ClassifierChain, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, ClassifierChain, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, ClassifierChain, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, ClassifierChain, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LabelPowerset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, LabelPowerset, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, LabelPowerset, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, LabelPowerset, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, LabelPowerset, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, LabelPowerset, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "prec_score, rec_score, f_score, ham_loss, conf_matrix = build_model(model, LabelPowerset, X, y)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Precision: \" + str(np.mean(prec_score)))\n",
    "print(\"Recall Score: \" + str(np.mean(rec_score)))\n",
    "print(\"F1 Score: \" + str(np.mean(f_score)))\n",
    "print(\"Hamming Loss: \" + str(np.mean(ham_loss)))\n",
    "plot_confusion_matrix(np.mean(conf_matrix, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Classifiers results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('./resultsMultilabel/resultsMULTILABELS.xlsx', mode='a',if_sheet_exists='replace') as writer:\n",
    "    results.to_excel(writer, sheet_name='MultiLabelClf BERT')\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "  df_pred1.to_excel(writer, sheet_name='Fold 1')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred2.to_excel(writer, sheet_name='Fold 2')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred3.to_excel(writer, sheet_name='Fold 3')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred4.to_excel(writer, sheet_name='Fold 4')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred5.to_excel(writer, sheet_name='Fold 5')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred6.to_excel(writer, sheet_name='Fold 6')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred6.to_excel(writer, sheet_name='Fold 6')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred7.to_excel(writer, sheet_name='Fold 7')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred8.to_excel(writer, sheet_name='Fold 8')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred9.to_excel(writer, sheet_name='Fold 9')\n",
    "with pd.ExcelWriter('./resultsMultilabel/multilabel-clf-bert.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "  df_pred10.to_excel(writer, sheet_name='Fold 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
